#!/bin/bash

TRAIN_DATASET=("hkust-nlp/drkernel-rl-data")    # you need to set your own training dataset
VALID_DATASET=("hkust-nlp/drkernel-validation-data")    # you need to set your own validation dataset
KERNELGYM_SERVER_URL="${KERNELGYM_SERVER_URL:-""}"   # set directly or via env
MODEL_NAME=hkust-nlp/drkernel-14b
MODEL_PATH=${MODEL_NAME}

RUN_NAME="drkernel-14b"
REWARD_MANAGER=kernel_async
REWARD_FUNC_NAME="calculate_reward_speedup"


ALGORITHM="trloo"

SPEEDUP_REWARD_UPPER_BOUND=3.0
SPEEDUP_REWARD_LOWER_BOUND=0.0

ROLLOUT_RS="geometric"
ROLLOUT_TOKEN_VETO_THRESHOLD=1e-4
ROLLOUT_RS_KWARGS="{lower:0.999,upper:1.001}"

COVERAGE_RS="turn"
COVERAGE_RS_THRESHOLD=0.3
COVERAGE_RS_FACTOR=0.1
COVERAGE_RS_KEY="time_coverage"

COVERAGE_REWARD_TYPE="time_coverage"
COVERAGE_REWARD_WEIGHT=0.5
COVERAGE_REWARD_ENABLE=True

REWARD_TASK_TIMEOUT=300
REWARD_TIMEOUT=1800
REWARD_ACQUIRE_TIMEOUT=2400
REWARD_MAX_CONCURRENT=32
REWARD_MAX_RETRIES=3
REWARD_PRINT_STATUS=True
NUM_PERF_TRIALS=100
REWARD_TASK_TIMEOUT_CLIENT=2400

VAL_BEFORE_TRAIN=True
IS_GET_LAST_TURN=True

# SWE-Agent-specific parameter overrides
ENABLE_MULTI_TURN=True
MAX_TURN=3
N_VAL=8
ACTOR_OPTIMIZER_OFFLOAD=True
ACTOR_PARAMETER_OFFLOAD=True
LEARNING_RATE=1e-6

TRAIN_BATCH_SIZE=16
PPO_MINI_BATCH_SIZE=16

AUTOMATIC_OVERSAMPLING=False
REJECTION_SAMPLE=True

PPO_MICRO_TOKEN=null
CLIP_RATIO=0.2_0.28
ENTROPY_CLIP_RATE=0.0
GRAD_CLIP=1.0
VLLM_IS_THRESHOLD=2.0
EXTREME_RISK_PROB_THRESHOLD=null
KL_LOSS_COEF=0.0
ENTROPY_COEFFIENT=0.0
KL_LOSS_TYPE="low_var_kl"
TEMPERATURE=1.0
MIN_P=0.0
TOP_P=1.0
TOP_K=-1
ROLLOUT_N=16
KL_COEF=0.0
TOTAL_EPOCHS=1000
ROLLOUT_GPU_MEMORY_UTIL=0.75

SAVE_FREQ=10
TEST_FREQ=10
ROLLOUT_TENSOR_MODEL_PARALLEL_SIZE=1
SP_SIZE=4
NUM_PERF_TRIALS=100
APPLY_CHAT_TEMPLATE=True
FREE_CACHE_ENGINE=False
ENFORCE_EAGER=False
NNODES=$ARNOLD_WORKER_NUM
GPUS_PER_NODE=$ARNOLD_WORKER_GPU
if [ -z "$ARNOLD_WORKER_GPU" ]; then
    GPUS_PER_NODE=8
fi

MAX_PROMPT_LENGTH=10240
MAX_RESPONSE_LENGTH=8192
PROMPT_OVERSAMPLING_FACTOR=1.0
SAMPLE_OVERSAMPLING_FACTOR=1.0
SAMPLE_SELECTION_STRATEGY=efficiency_stochastic
MAX_SKIP_STEPS=5

# Load and execute common script
source "$(dirname "$0")/train_rl_common.sh"

# Call main function with all arguments
main "$@"
